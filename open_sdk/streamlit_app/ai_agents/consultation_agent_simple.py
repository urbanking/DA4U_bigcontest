"""
ê°„ì†Œí™”ëœ ìƒë‹´ ì—ì´ì „íŠ¸ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìµœì í™”
"""
import json
from pathlib import Path
from typing import Dict, Any, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langfuse.decorators import observe
import os
from dotenv import load_dotenv

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

def load_agent_results(store_code: str) -> dict:
    """
    ì—ì´ì „íŠ¸ ê²°ê³¼ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ íŒŒì‹±
    
    Args:
        store_code: ë§¤ì¥ ì½”ë“œ
        
    Returns:
        dict: íŒŒì‹±ëœ ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤
    """
    results = {
        "marketing_result": None,
        "new_product_result": None
    }
    
    try:
        from pathlib import Path
        
        # output í´ë”ì—ì„œ í•´ë‹¹ store_codeì˜ ìµœì‹  ë¶„ì„ í´ë” ì°¾ê¸°
        output_dir = Path(__file__).parent.parent.parent / "output"
        store_folders = sorted(
            [f for f in output_dir.glob(f"analysis_{store_code}_*") if f.is_dir()],
            key=lambda x: x.name,
            reverse=True
        )
        
        if store_folders:
            latest_folder = store_folders[0]
            
            # Marketing Agent ê²°ê³¼ ë¡œë“œ
            marketing_file = latest_folder / "marketing_result.json"
            if marketing_file.exists():
                with open(marketing_file, 'r', encoding='utf-8') as f:
                    results["marketing_result"] = json.load(f)
                print(f"[DEBUG] Marketing result loaded: {marketing_file.name}")
            
            # New Product Agent ê²°ê³¼ ë¡œë“œ
            new_product_file = latest_folder / "new_product_result.json"
            if new_product_file.exists():
                with open(new_product_file, 'r', encoding='utf-8') as f:
                    results["new_product_result"] = json.load(f)
                print(f"[DEBUG] New Product result loaded: {new_product_file.name}")
                
    except Exception as e:
        print(f"[WARN] Failed to load agent results: {e}")
    
    return results

def create_consultation_chain(store_code: str, analysis_data: Dict[str, Any], analysis_md: str, mcp_content: str) -> Tuple[RunnableWithMessageHistory, BaseChatMessageHistory]:
    """
    ê°„ì†Œí™”ëœ Langchain Consultation Chain ìƒì„±
    """
    try:
        print(f"[DEBUG] Creating consultation chain for store {store_code}")
        
        # Gemini ëª¨ë¸ ì´ˆê¸°í™” (ì¶œë ¥ í† í° ìˆ˜ ì¦ê°€)
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=GEMINI_API_KEY,
            temperature=0.7,
            max_output_tokens=8192  # í† í° ìˆ˜ ëŒ€í­ ì¦ê°€
        )
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        chat_history = InMemoryChatMessageHistory()
        
        # ë¶„ì„ ë°ì´í„°ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        store_name = analysis_data.get("store_analysis", {}).get("store_overview", {}).get("name", "N/A")
        industry = analysis_data.get("store_analysis", {}).get("store_overview", {}).get("industry", "N/A")
        commercial_area = analysis_data.get("store_analysis", {}).get("store_overview", {}).get("commercial_area", "N/A")
        
        # ì—ì´ì „íŠ¸ ê²°ê³¼ ë¡œë“œ
        agent_results = load_agent_results(store_code)
        
        # ë§ˆì¼€íŒ… ì „ëµ ê°„ì†Œí™”
        marketing_strategies = []
        if agent_results.get("marketing_result"):
            marketing_strategies = agent_results["marketing_result"].get("marketing_strategies", [])
        else:
            marketing_strategies = analysis_data.get("marketing_analysis", {}).get("marketing_strategies", [])
        
        strategy_summary = "\n".join([
            f"- {i+1}. **{s.get('name', 'N/A')}**: {s.get('description', 'N/A')[:80]}..."
            for i, s in enumerate(marketing_strategies[:3])
        ]) if marketing_strategies else "ë§ˆì¼€íŒ… ì „ëµ ì •ë³´ ì—†ìŒ"
        
        # ì‹ ì œí’ˆ ì œì•ˆ ê°„ì†Œí™”
        new_product_proposals = []
        if agent_results.get("new_product_result") and agent_results["new_product_result"].get("activated"):
            new_product_proposals = agent_results["new_product_result"].get("proposals", [])
        
        new_product_summary = "\n".join([
            f"- {i+1}. **{p.get('menu_name', 'N/A')}** ({p.get('category', 'N/A')}) - íƒ€ê²Ÿ: {p.get('target', {}).get('gender', 'N/A')} {', '.join(p.get('target', {}).get('ages', []))}"
            for i, p in enumerate(new_product_proposals[:3])
        ]) if new_product_proposals else "ì‹ ì œí’ˆ ì œì•ˆ ì •ë³´ ì—†ìŒ"
        
        # íŒŒë…¸ë¼ë§ˆ ë¶„ì„ ê°„ì†Œí™”
        panorama_summary = analysis_data.get("panorama_analysis", {}).get("synthesis", {}).get("final_recommendation", "N/A")
        
        # MCP ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        safe_mcp_content = ""
        if mcp_content:
            safe_mcp_content = mcp_content[:1000].replace("{", "{{").replace("}", "}}")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°„ì†Œí™”
        system_prompt = f"""ë‹¹ì‹ ì€ ë§¤ì¥ '{store_name}' (ì—…ì¢…: {industry}, ìƒê¶Œ: {commercial_area})ì˜ ì „ë¬¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“Š í•µì‹¬ ë¶„ì„ ë°ì´í„°
{safe_mcp_content}

### ğŸ“ˆ ë§ˆì¼€íŒ… ì „ëµ
{strategy_summary}

### ğŸ° ì‹ ì œí’ˆ ì œì•ˆ
{new_product_summary}

### ğŸŒ† ì§€ì—­ íŠ¹ì„±
{panorama_summary[:150]}...

## ğŸ’¡ ë‹µë³€ ê°€ì´ë“œë¼ì¸
- **JSON ë°ì´í„° ê¸°ë°˜**ìœ¼ë¡œ ì •í™•í•œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
- **ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸**ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”  
- **ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µ**í•˜ê³  ì—°ì†ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- **ê´€ë ¨ ì—†ëŠ” ë©”ë‰´ ì œì•ˆ ê¸ˆì§€** - ì‹¤ì œ ë°ì´í„°ì— ê·¼ê±°í•œ ì œì•ˆë§Œ í•˜ì„¸ìš”

## ğŸš« ê¸ˆì§€ ì‚¬í•­
- ì¶”ì¸¡ì„± ë‹µë³€, ë°ì´í„° ì—†ëŠ” ì •ë³´ ì œì‹œ ê¸ˆì§€
- ë¯¼ê°í•œ ê²½ì˜ ì •ë³´ ì§ì ‘ ì–¸ê¸‰ ê¸ˆì§€
"""
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        # Chain ìƒì„±
        chain = prompt_template | llm
        
        print(f"[OK] Consultation chain created successfully")
        return chain, chat_history
        
    except Exception as e:
        print(f"[ERROR] Failed to create consultation chain: {e}")
        import traceback
        traceback.print_exc()
        return None, None

@observe()
def chat_with_consultant(chain, chat_history, user_message: str, store_data: dict = None, panorama_data: dict = None) -> str:
    """
    ìƒë‹´ ì²´ì¸ê³¼ ëŒ€í™” (ê°„ì†Œí™” ë²„ì „)
    """
    try:
        print(f"[DEBUG] User: {user_message}")
        
        # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        history_messages = chat_history.messages
        
        # ì²´ì¸ ì‹¤í–‰
        response = chain.invoke({
            "chat_history": history_messages,
            "input": user_message
        })
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        chat_history.add_user_message(user_message)
        chat_history.add_ai_message(response.content)
        
        return response.content
    except Exception as e:
        print(f"[ERROR] Error during chat_with_consultant: {e}")
        import traceback
        traceback.print_exc()
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ìƒë‹´ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
