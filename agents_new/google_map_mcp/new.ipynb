{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a7183e",
   "metadata": {},
   "source": [
    "# AI Marketing Agent — MCP crawl of matched_store_results.csv\n",
    "\n",
    "This notebook loads environment variables from `agents_new/.env`, reads `matched_store_results.csv`, builds search queries (matched name + address), calls the MCP-enabled `GoogleMapsAgent.search_place` for each row (or runs in `dry_run` mode to only print queries), and saves per-store JSON and an aggregate CSV `outputs/matched_stores_mcp_results.csv`.\n",
    "\n",
    "Use the `DRY_RUN = True` flag to test locally without external calls. Set to `False` for a real crawl (ensure your environment and MCP server are ready)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d745cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading env from: c:\\ㅈ\\DA4U\\bigcontest_ai_agent\\agents_new\\google_map_mcp\\..\\.env\n",
      "Environment keys visible (sample): {'Google_Map_API_KEY': 'AIzaSyCWHD5C8-daqFpwK_FPoq0SfIrLrEK8iX0', 'GEMINI_API_KEY': 'AIzaSyC7VghrgTrBF2Ag1J1KGPRWrltDD4Skg0Y'}\n",
      "Processing rows 0..2 (total 4185)\n",
      "[dry-run] 0/3 code=000F03E44A query=육육면관 대한민국 서울특별시 성동구 왕십리로4가길 9\n",
      "[dry-run] 1/3 code=002816BA73 query=마장동 할머니 갈비탕 대한민국 서울특별시 성동구 청계천로10가길 10-7\n",
      "[dry-run] 2/3 code=003473B465 query=Bottle5150 바틀5150 대한민국 서울특별시 성동구 서울숲길 55\n",
      "Crawl finished.\n"
     ]
    }
   ],
   "source": [
    "# Single-cell crawler implementation\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Helper: load .env-like files into os.environ (simple parser)\n",
    "def load_env_file(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Env file not found: {path}\")\n",
    "        return\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if '=' not in line:\n",
    "                continue\n",
    "            k, v = line.split('=', 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip()\n",
    "            # remove optional surrounding quotes\n",
    "            if (v.startswith('\"') and v.endswith('\"')) or (v.startswith(\"'\") and v.endswith(\"'\")):\n",
    "                v = v[1:-1]\n",
    "            os.environ.setdefault(k, v)\n",
    "\n",
    "# Load env from repository-level agents_new/.env (matches GoogleMapsAgent behavior)\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# try common locations\n",
    "candidates = [\n",
    "    os.path.join(os.getcwd(), '.env'),\n",
    "    os.path.join(os.getcwd(), '..', '.env'),\n",
    "    os.path.join(os.getcwd(), '..', '..', 'agents_new', '.env'),\n",
    "    os.path.join(os.getcwd(), '..', '..', '.env'),\n",
    "    os.path.join(os.getcwd(), '..', 'agents_new', '.env'),\n",
    "]\n",
    "# Also try the exact path relative to this notebook (agents_new/.env)\n",
    "candidates.insert(0, os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..', '.env'))\n",
    "\n",
    "loaded_any = False\n",
    "for p in candidates:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Loading env from: {p}\")\n",
    "        load_env_file(p)\n",
    "        loaded_any = True\n",
    "        break\n",
    "if not loaded_any:\n",
    "    # fallback: try the workspace agents_new/.env\n",
    "    fallback = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'agents_new', '.env'))\n",
    "    if os.path.exists(fallback):\n",
    "        print(f\"Loading env from fallback: {fallback}\")\n",
    "        load_env_file(fallback)\n",
    "        loaded_any = True\n",
    "\n",
    "print('Environment keys visible (sample):', {k: os.getenv(k) for k in ['Google_Map_API_KEY','GEMINI_API_KEY']})\n",
    "\n",
    "# Import the agent (after env is loaded)\n",
    "from google_maps_agent import GoogleMapsAgent\n",
    "\n",
    "# Build query from CSV row\n",
    "def build_query(row: Dict[str, str]) -> str:\n",
    "    name = (row.get('매칭_상호명') or row.get('입력_가맹점명') or '').strip()\n",
    "    address = (row.get('매칭_주소') or row.get('입력_주소') or '').strip()\n",
    "    parts = [p for p in (name, address) if p]\n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Main crawl logic\n",
    "def crawl_matched_csv(csv_path: str, out_dir: str, dry_run: bool = True, start: int = 0, limit: int = None, sleep_s: float = 1.0, force: bool = False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    aggregate_csv = os.path.join(out_dir, 'matched_stores_mcp_results.csv')\n",
    "    json_dir = out_dir\n",
    "\n",
    "    with open(csv_path, newline='', encoding='utf-8-sig') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "\n",
    "    total = len(rows)\n",
    "    end = total if limit is None else min(total, start + limit)\n",
    "    print(f\"Processing rows {start}..{end-1} (total {total})\")\n",
    "\n",
    "    # Prepare CSV header\n",
    "    fieldnames = ['코드','입력_가맹점명','입력_주소','매칭_상호명','매칭_주소','place_id','success','mcp_text_file','mcp_text_snippet','error']\n",
    "    # If not force and aggregate exists, load already processed codes to skip\n",
    "    processed_codes = set()\n",
    "    if os.path.exists(aggregate_csv) and not force:\n",
    "        try:\n",
    "            with open(aggregate_csv, newline='', encoding='utf-8-sig') as agf:\n",
    "                agreader = csv.DictReader(agf)\n",
    "                for r in agreader:\n",
    "                    processed_codes.add(r.get('코드'))\n",
    "            print(f\"Found {len(processed_codes)} previously processed rows; they will be skipped unless --force\")\n",
    "        except Exception as e:\n",
    "            print('Warning reading existing aggregate CSV:', e)\n",
    "\n",
    "    # Instantiate agent only for live runs\n",
    "    agent = None\n",
    "    if not dry_run:\n",
    "        agent = GoogleMapsAgent()\n",
    "\n",
    "    # Open aggregate CSV for append or create\n",
    "    new_file = not os.path.exists(aggregate_csv) or force\n",
    "    agf = open(aggregate_csv, 'a', newline='', encoding='utf-8')\n",
    "    writer = csv.DictWriter(agf, fieldnames=fieldnames)\n",
    "    if new_file:\n",
    "        writer.writeheader()\n",
    "\n",
    "    try:\n",
    "        for idx in range(start, end):\n",
    "            row = rows[idx]\n",
    "            code = row.get('코드') or f'row_{idx}'\n",
    "            if (code in processed_codes) and not force:\n",
    "                print(f\"Skipping {code} (already processed)\")\n",
    "                continue\n",
    "            query = build_query(row)\n",
    "            out_filename = os.path.join(json_dir, f'mcp_{code}.json')\n",
    "\n",
    "            rec = {\n",
    "                '코드': code,\n",
    "                '입력_가맹점명': row.get('입력_가맹점명'),\n",
    "                '입력_주소': row.get('입력_주소'),\n",
    "                '매칭_상호명': row.get('매칭_상호명'),\n",
    "                '매칭_주소': row.get('매칭_주소'),\n",
    "                'place_id': row.get('place_id'),\n",
    "                'success': False,\n",
    "                'mcp_text_file': '',\n",
    "                'mcp_text_snippet': '',\n",
    "                'error': '',\n",
    "            }\n",
    "\n",
    "            if dry_run:\n",
    "                print(f\"[dry-run] {idx}/{end} code={code} query={query}\")\n",
    "                rec['mcp_text_snippet'] = query[:200]\n",
    "                writer.writerow(rec)\n",
    "                agf.flush()\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"[{idx+1}/{end}] Querying: {query}\")\n",
    "                mcp_text = agent.search_place(query)\n",
    "                rec['success'] = True\n",
    "                # Save full text to per-store JSON\n",
    "                rec['mcp_text_file'] = out_filename\n",
    "                rec['mcp_text_snippet'] = (mcp_text or '')[:300]\n",
    "                with open(out_filename, 'w', encoding='utf-8') as of:\n",
    "                    json.dump({'query': query, 'mcp_text': mcp_text, 'row': row}, of, ensure_ascii=False, indent=2)\n",
    "                writer.writerow(rec)\n",
    "                agf.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {code}: {e}\")\n",
    "                rec['error'] = str(e)\n",
    "                writer.writerow(rec)\n",
    "                agf.flush()\n",
    "\n",
    "            time.sleep(sleep_s)\n",
    "    finally:\n",
    "        agf.close()\n",
    "\n",
    "    print('Crawl finished.')\n",
    "\n",
    "# Example run: dry-run for first 3 rows\n",
    "CSV_PATH = 'matched_store_results.csv'\n",
    "OUT_DIR = 'outputs'\n",
    "\n",
    "# Change these flags to run live\n",
    "DRY_RUN = True\n",
    "START = 0\n",
    "LIMIT = 3  # set None to process all rows\n",
    "SLEEP = 1.0\n",
    "FORCE = False\n",
    "\n",
    "crawl_matched_csv(CSV_PATH, OUT_DIR, dry_run=DRY_RUN, start=START, limit=LIMIT, sleep_s=SLEEP, force=FORCE)\n",
    "\n",
    "# If you want to run the full live crawl, set:\n",
    "# DRY_RUN = False; LIMIT = None; then re-run the cell (ensure env and MCP/server are ready)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
